{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "drru_state_critic.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0P6UT1UAjIrA",
        "outputId": "8da0519d-62c0-45c8-e285-ff17ab3dc5bf"
      },
      "source": [
        "!pip install scikit-surprise"
      ],
      "execution_count": 90,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: scikit-surprise in /usr/local/lib/python3.7/dist-packages (1.1.1)\n",
            "Requirement already satisfied: scipy>=1.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-surprise) (1.4.1)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-surprise) (1.0.1)\n",
            "Requirement already satisfied: numpy>=1.11.2 in /usr/local/lib/python3.7/dist-packages (from scikit-surprise) (1.19.5)\n",
            "Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.7/dist-packages (from scikit-surprise) (1.15.0)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-4HBqF5Vj8Gj"
      },
      "source": [
        "from surprise import Dataset\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import random"
      ],
      "execution_count": 106,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3CqVA9ye4r_0"
      },
      "source": [
        "class Env:\n",
        "  def __init__(self, users_dict, users_history_lens, state_size, fix_user_id=None):\n",
        "\n",
        "    self.users_dict = users_dict\n",
        "    self.users_history_lens = users_history_lens\n",
        "    self.state_size = state_size\n",
        "\n",
        "    self.fix_user_id = fix_user_id\n",
        "\n",
        "    self.available_users = self._generate_available_users()\n",
        "\n",
        "    self.user = self.fix_user_id if self.fix_user_id else np.random.choice(self.available_users)\n",
        "    self.user_items = {data[0]:data[1] for data in self.users_dict[self.user]}\n",
        "    self.items = [data[0] for data in self.users_dict[self.user][:self.state_size]]\n",
        "    self.done = False\n",
        "\n",
        "    self.recommended_items = set(self.items)\n",
        "    self.done_count = 3000\n",
        "\n",
        "  def _generate_available_users(self):\n",
        "    available_users = []\n",
        "\n",
        "    for i, length in zip(self.users_dict.keys(), self.users_history_lens):\n",
        "      if length > self.state_size:\n",
        "        available_users.append(i)\n",
        "\n",
        "    return available_users\n",
        "\n",
        "  def reset(self):\n",
        "    self.user = self.fix_user_id if self.fix_user_id else np.random.choice(self.available_users)\n",
        "    self.user_items = {data[0]:data[1] for data in self.users_dict[self.user]}\n",
        "    self.items = [data[0] for data in self.users_dict[self.user][:self.state_size]]\n",
        "    self.done = False\n",
        "    self.recommended_items = set(self.items)\n",
        "    return self.user, self.items, self.done\n",
        "\n",
        "  def step(self, action, top_k=False):\n",
        "    reward = -0.5\n",
        "\n",
        "    if top_k:\n",
        "      pass\n",
        "    else:\n",
        "      print('action:', action)\n",
        "      if action in self.user_items.keys() and action not in self.recommended_items:\n",
        "        reward = self.user_items[action] - 3\n",
        "      if reward > 0:\n",
        "        self.items = self.items[1:] + [action]\n",
        "      self.recommended_items.add(action)\n",
        "\n",
        "    if len(self.recommended_items) > self.done_count or len(self.recommended_items) >= self.users_history_lens[self.user]: #-1??\n",
        "      self.done = True\n",
        "\n",
        "    return self.items, reward, self.done, self.recommended_items\n",
        "\n",
        "  "
      ],
      "execution_count": 92,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Hon5eBAB6PhH"
      },
      "source": [
        "class UserMovieEmbedding(nn.Module):\n",
        "    def __init__(self, n_users, n_movies, n_factors = 100, nh = 20, p1 = 0.05, p2= 0.5):\n",
        "        super().__init__()\n",
        "        self.u = nn.Embedding(n_users, n_factors)\n",
        "        self.u.weight.data.uniform_(-0.01,0.01)\n",
        "        self.m = nn.Embedding(n_movies, n_factors)\n",
        "        self.m.weight.data.uniform_(-0.01,0.01)\n",
        "        self.lin1 = nn.Linear(n_factors*2, nh)  # bias is True by default\n",
        "        self.lin2 = nn.Linear(nh, 1)\n",
        "        self.drop1 = nn.Dropout(p = p1)\n",
        "        self.drop2 = nn.Dropout(p = p2)\n",
        "    \n",
        "    def forward(self, users, movies): # forward pass i.e.  dot product of vector from movie embedding matrixx\n",
        "                                    # and vector from user embeddings matrix\n",
        "        \n",
        "        # torch.cat : concatenates both embedding matrix to make more columns, same rows i.e. n_factors*2, n : rows\n",
        "        # u(users) is doing lookup for indexed mentioned in users\n",
        "        # users has indexes to lookup in embedding matrix. \n",
        "        \n",
        "        u2,m2 = self.u(users) , self.m(movies)\n",
        "       \n",
        "        x = self.drop1(torch.cat([u2,m2], 1)) # drop initialized weights\n",
        "        x = self.drop2(F.relu(self.lin1(x))) # drop 1st linear + nonlinear wt\n",
        "        r = torch.sigmoid(self.lin2(x)) * (max_rating - min_rating) + min_rating               \n",
        "        return r\n",
        "    \n",
        "    \n",
        "    def get_user_embedding_layer(self, user):\n",
        "        return self.u\n",
        "    \n",
        "    def get_movie_embedding_layer(self, user):\n",
        "        return self.m"
      ],
      "execution_count": 93,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cJqX17WS84uj"
      },
      "source": [
        "class InnerProductLayer(nn.Module):\n",
        "    \"\"\"InnerProduct Layer used in PNN that compute the element-wise\n",
        "    product or inner product between feature vectors.\n",
        "      Input shape\n",
        "        - a list of 3D tensor with shape: ``(batch_size,1,embedding_size)``.\n",
        "      Output shape\n",
        "        - 3D tensor with shape: ``(batch_size, N*(N-1)/2 ,1)`` if use reduce_sum. or 3D tensor with shape:\n",
        "        ``(batch_size, N*(N-1)/2, embedding_size )`` if not use reduce_sum.\n",
        "      Arguments\n",
        "        - **reduce_sum**: bool. Whether return inner product or element-wise product\n",
        "      References\n",
        "            - [Qu Y, Cai H, Ren K, et al. Product-based neural networks for user response prediction[C]//\n",
        "            Data Mining (ICDM), 2016 IEEE 16th International Conference on. IEEE, 2016: 1149-1154.]\n",
        "            (https://arxiv.org/pdf/1611.00144.pdf)\"\"\"\n",
        "\n",
        "    def __init__(self, num_inputs, device='cpu'):\n",
        "        super(InnerProductLayer, self).__init__()\n",
        "        self.W = nn.Parameter(torch.diag(torch.rand((num_inputs,1))))\n",
        "        self.W.requires_grad = True\n",
        "        self.to(device)\n",
        "\n",
        "    def forward(self, inputs, user):\n",
        "\n",
        "        embed_list = inputs\n",
        "        row = []\n",
        "        col = []\n",
        "        num_inputs = len(embed_list)\n",
        "\n",
        "        print('num_inputs', num_inputs)\n",
        "\n",
        "        embed_list = torch.matmul(self.W, embed_list)\n",
        "        embed_list = embed_list.unsqueeze(1)\n",
        "\n",
        "        # create all pairs of item embeddings\n",
        "        #and after that multiply element-wise\n",
        "        for i in range(num_inputs - 1):\n",
        "            for j in range(i + 1, num_inputs):\n",
        "                row.append(i)\n",
        "                col.append(j)\n",
        "\n",
        "        p = torch.cat([embed_list[idx]\n",
        "                       for idx in row], dim=1)  # batch num_pairs k\n",
        "        q = torch.cat([embed_list[idx]\n",
        "                       for idx in col], dim=1)\n",
        "\n",
        "        #multiply element-wise the user embedding with all items\n",
        "        u = user * embed_list\n",
        "\n",
        "        inner_product = p * q\n",
        "\n",
        "        u = u.reshape(-1).unsqueeze(0)\n",
        "        result = torch.cat((u, inner_product), dim=1)\n",
        "        print('result: ', result.shape)\n",
        "        return result"
      ],
      "execution_count": 94,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xgoDRT2X1ajx"
      },
      "source": [
        "import numpy as np\n",
        "from collections import deque\n",
        "\n",
        "class SumTree:\n",
        "    def __init__(self, buffer_size):\n",
        "        self.buffer_size = buffer_size\n",
        "        self.tree = np.zeros((buffer_size * 2 - 1))\n",
        "        self.index = buffer_size - 1\n",
        "\n",
        "    def update_tree(self, index):\n",
        "        while True:\n",
        "            index = (index - 1) // 2\n",
        "            left = (index * 2) + 1\n",
        "            right = (index * 2) + 2\n",
        "            self.tree[index] = self.tree[left] + self.tree[right]\n",
        "            if index == 0:\n",
        "                break\n",
        "\n",
        "    def add_data(self, priority):\n",
        "        if self.index == self.buffer_size * 2 - 1:\n",
        "            self.index = self.buffer_size - 1\n",
        "\n",
        "        self.tree[self.index] = priority\n",
        "        self.update_tree(self.index)\n",
        "        self.index += 1\n",
        "\n",
        "    def search(self, num):\n",
        "        current = 0\n",
        "        while True:\n",
        "            left = (current * 2) + 1\n",
        "            right = (current * 2) + 2\n",
        "\n",
        "            if num <= self.tree[left]:\n",
        "                current = left\n",
        "            else:\n",
        "                num -= self.tree[left]\n",
        "                current = right\n",
        "            \n",
        "            if current >= self.buffer_size - 1:\n",
        "                break\n",
        "\n",
        "        return self.tree[current], current, current - self.buffer_size + 1\n",
        "\n",
        "    def update_prioirty(self, priority, index):\n",
        "        self.tree[index] = priority\n",
        "        self.update_tree(index)\n",
        "\n",
        "    def sum_all_prioirty(self):\n",
        "        return float(self.tree[0])\n",
        "\n",
        "\n",
        "class MinTree:\n",
        "    def __init__(self, buffer_size):\n",
        "        self.buffer_size = buffer_size\n",
        "        self.tree = np.ones((buffer_size * 2 - 1))\n",
        "        self.index = buffer_size - 1\n",
        "\n",
        "    def update_tree(self, index):\n",
        "        while True:\n",
        "            index = (index - 1) // 2\n",
        "            left = (index * 2) + 1\n",
        "            right = (index * 2) + 2\n",
        "            if self.tree[left] > self.tree[right]:\n",
        "                self.tree[index] = self.tree[right]\n",
        "            else:\n",
        "                self.tree[index] = self.tree[left]\n",
        "            if index == 0:\n",
        "                break\n",
        "\n",
        "    def add_data(self, priority):\n",
        "        if self.index == self.buffer_size * 2 - 1:\n",
        "            self.index = self.buffer_size - 1\n",
        "\n",
        "        self.tree[self.index] = priority\n",
        "        self.update_tree(self.index)\n",
        "        self.index += 1\n",
        "\n",
        "    def update_prioirty(self, priority, index):\n",
        "        self.tree[index] = priority\n",
        "        self.update_tree(index)\n",
        "\n",
        "    def min_prioirty(self):\n",
        "        return float(self.tree[0])\n",
        "\n",
        "class PriorityExperienceReplay(object):\n",
        "\n",
        "    '''\n",
        "    apply PER\n",
        "    '''\n",
        "\n",
        "    def __init__(self, buffer_size, embedding_dim, state_size):\n",
        "        self.buffer_size = buffer_size\n",
        "        self.crt_idx = 0\n",
        "        self.is_full = False\n",
        "        \n",
        "        '''\n",
        "            state : (300,), \n",
        "            next_state : (300,) 변할 수 잇음, \n",
        "            actions : (100,), \n",
        "            rewards : (1,), \n",
        "            dones : (1,)\n",
        "        '''\n",
        "        self.states = torch.zeros((buffer_size, state_size))\n",
        "        self.actions = torch.zeros((buffer_size, embedding_dim))\n",
        "        self.rewards = torch.zeros((buffer_size))\n",
        "        self.next_states = torch.zeros((buffer_size, state_size))\n",
        "        self.dones = torch.zeros(buffer_size)\n",
        "\n",
        "        self.sum_tree = SumTree(buffer_size)\n",
        "        self.min_tree = MinTree(buffer_size)\n",
        "\n",
        "        self.max_prioirty = 1.0\n",
        "        self.alpha = 0.6\n",
        "        self.beta = 0.4\n",
        "        self.beta_constant = 0.00001\n",
        "\n",
        "    def append(self, state, action, reward, next_state, done):\n",
        "        self.states[self.crt_idx] = state\n",
        "        self.actions[self.crt_idx] = action\n",
        "        self.rewards[self.crt_idx] = reward\n",
        "        self.next_states[self.crt_idx] = next_state\n",
        "        self.dones[self.crt_idx] = done\n",
        "\n",
        "        self.sum_tree.add_data(self.max_prioirty ** self.alpha)\n",
        "        self.min_tree.add_data(self.max_prioirty ** self.alpha)\n",
        "        \n",
        "        self.crt_idx = (self.crt_idx + 1) % self.buffer_size\n",
        "        if self.crt_idx == 0:\n",
        "            self.is_full = True\n",
        "\n",
        "    def sample(self, batch_size):\n",
        "        rd_idx = []\n",
        "        weight_batch = []\n",
        "        index_batch = []\n",
        "        sum_priority = self.sum_tree.sum_all_prioirty()\n",
        "        \n",
        "        N = self.buffer_size if self.is_full else self.crt_idx\n",
        "        min_priority = self.min_tree.min_prioirty() / sum_priority\n",
        "        max_weight = (N * min_priority) ** (-self.beta)\n",
        "\n",
        "        segment_size = sum_priority/batch_size\n",
        "        for j in range(batch_size):\n",
        "            min_seg = segment_size * j\n",
        "            max_seg = segment_size * (j + 1)\n",
        "\n",
        "            random_num = random.uniform(min_seg, max_seg)\n",
        "            priority, tree_index, buffer_index = self.sum_tree.search(random_num)\n",
        "            rd_idx.append(buffer_index)\n",
        "\n",
        "            p_j = priority / sum_priority\n",
        "            w_j = (p_j * N) ** (-self.beta) / max_weight\n",
        "            weight_batch.append(w_j)\n",
        "            index_batch.append(tree_index)\n",
        "        self.beta = min(1.0, self.beta + self.beta_constant)\n",
        "\n",
        "        batch_states = self.states[rd_idx]\n",
        "        batch_actions = self.actions[rd_idx]\n",
        "        batch_rewards = self.rewards[rd_idx]\n",
        "        batch_next_states = self.next_states[rd_idx]\n",
        "        batch_dones = self.dones[rd_idx]\n",
        "\n",
        "        return batch_states, batch_actions, batch_rewards, batch_next_states, batch_dones, torch.Tensor(weight_batch), index_batch\n",
        "\n",
        "    def update_priority(self, priority, index):\n",
        "        self.sum_tree.update_prioirty(priority ** self.alpha, index)\n",
        "        self.min_tree.update_prioirty(priority ** self.alpha, index)\n",
        "        self.update_max_priority(priority ** self.alpha)\n",
        "\n",
        "    def update_max_priority(self, priority):\n",
        "        self.max_prioirty = max(self.max_prioirty, priority)"
      ],
      "execution_count": 121,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "caEplGZgLDxt"
      },
      "source": [
        "class CriticNetwork(nn.Module):\n",
        "  def __init__(self, state_size, action_size):\n",
        "    super(CriticNetwork, self).__init__()\n",
        "\n",
        "    self.lin1 = nn.Linear(state_size, state_size)\n",
        "    self.lin2 = nn.Linear(state_size+action_size, state_size)\n",
        "    self.lin3 = nn.Linear(state_size, 1)\n",
        "\n",
        "    self.relu = nn.ReLU()\n",
        "\n",
        "  def forward(self, action, state):\n",
        "    state = self.relu(self.lin1(state))\n",
        "    input_concat = torch.cat((action, state), dim=1)\n",
        "\n",
        "    x = self.lin2(input_concat)\n",
        "    x = self.relu(x)\n",
        "    x = self.lin3(x)\n",
        "    x = self.relu(x)\n",
        "\n",
        "    return x\n",
        "\n",
        "class Critic:\n",
        "  def __init__(self, embedding_dim, hidden_dim, state_size, learning_rate, tau):\n",
        "    self.embedding_dim = embedding_dim\n",
        "\n",
        "    self.local_network = CriticNetwork(state_size, embedding_dim)\n",
        "    self.target_network = CriticNetwork(state_size, embedding_dim)\n",
        "\n",
        "    self.optimizer = torch.optim.Adam(self.local_network.parameters(),lr=learning_rate)\n",
        "    self.loss = nn.MSELoss()\n",
        "\n",
        "    self.tau = tau\n",
        "\n",
        "  def update_target_network(self):\n",
        "    \"\"\"Soft update model parameters.\n",
        "        θ_target = τ*θ_local + (1 - τ)*θ_target\n",
        "    \"\"\"\n",
        "\n",
        "    for target_param, local_param in zip(self.target_network.parameters(), self.local_network.parameters()):\n",
        "      target_param.data.copy_(self.tau*local_param.data + (1.0-self.tau)*target_param.data)\n",
        "\n",
        "\n",
        "  def dq_da(self, input):\n",
        "    \"\"\"\n",
        "      Gradient of Q at a\n",
        "    \"\"\"\n",
        "\n",
        "\n",
        "  def train(self, actions,states, td_targets, weight_batch):\n",
        "    with torch.autograd.set_detect_anomaly(True):\n",
        "      self.optimizer.zero_grad()\n",
        "      outputs = self.local_network(actions, states)\n",
        "      loss = self.loss(outputs, td_targets)\n",
        "      loss = torch.mean(weight_batch*loss)\n",
        "\n",
        "      loss.backward()\n",
        "      self.optimizer.step()\n",
        "      return loss"
      ],
      "execution_count": 125,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Yl1ukg_NMksp"
      },
      "source": [
        "class ActorNetwork(nn.Module):\n",
        "  def __init__(self, state_dim, hidden_dim, output_dim):\n",
        "        super().__init__()        \n",
        "        self.lin1 = nn.Linear(in_features=state_dim, out_features=hidden_dim)\n",
        "        self.lin2 = nn.Linear(in_features=hidden_dim, out_features=hidden_dim)\n",
        "        self.lin3 = nn.Linear(in_features=hidden_dim, out_features=output_dim)\n",
        "\n",
        "  def forward(self, state):\n",
        "        print('Actor state shape:', state.shape)\n",
        "        new_state = torch.relu(self.lin1(state))\n",
        "        new_state = torch.relu(self.lin2(new_state))\n",
        "        action = torch.tanh(self.lin3(new_state))\n",
        "        return action\n",
        "\n",
        "class Actor:\n",
        "  def __init__(self, embedding_dim, hidden_dim, state_size, learning_rate, tau):\n",
        "    self.embedding_dim = embedding_dim\n",
        "    self.state_size = state_size\n",
        "\n",
        "    self.local_network = ActorNetwork(state_size, hidden_dim, embedding_dim)\n",
        "    self.target_network = ActorNetwork(state_size, hidden_dim, embedding_dim)\n",
        "\n",
        "    self.optimizer = torch.optim.Adam(self.local_network.parameters(),lr=learning_rate)\n",
        "    self.loss = nn.MSELoss()\n",
        "\n",
        "    self.tau = tau\n",
        "\n",
        "  def train(self):\n",
        "    pass\n",
        "\n",
        "\n",
        "  def update_target_network(self):\n",
        "    \"\"\"Soft update model parameters.\n",
        "        θ_target = τ*θ_local + (1 - τ)*θ_target\n",
        "    \"\"\"\n",
        "\n",
        "    for target_param, local_param in zip(self.target_network.parameters(), self.local_network.parameters()):\n",
        "      target_param.data.copy_(self.tau*local_param.data + (1.0-self.tau)*target_param.data)"
      ],
      "execution_count": 97,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XzT7oxOCODHB"
      },
      "source": [
        "class Recommender:\n",
        "\n",
        "  EMBEDDING_DIM = 100\n",
        "\n",
        "  ACTOR_HIDDEN_DIM = 128\n",
        "  ACTOR_LR = 0.001\n",
        "  CRITIC_HIDDEN_DIM = 128\n",
        "  CRITIC_LR = 0.001\n",
        "\n",
        "  DISCOUNT_FACTOR = 0.9\n",
        "  TAU = 0.001\n",
        "\n",
        "  REPLAY_MEMORY_SIZE = 1000\n",
        "  BATCH_SIZE = 32\n",
        "\n",
        "  EPSILON_FOR_PRIORITY = 1e-6\n",
        "\n",
        "  def __init__(self, env, users, items, state_size, ):\n",
        "    self.env = env\n",
        "    self.users = users\n",
        "    self.items = items\n",
        "\n",
        "    self.state_size = int(self.EMBEDDING_DIM*(state_size + state_size*(state_size-1)/2)) #drr-u output dim\n",
        "\n",
        "    self.actor = Actor(self.EMBEDDING_DIM, self.ACTOR_HIDDEN_DIM, self.state_size, self.ACTOR_LR, self.TAU)\n",
        "    self.critic = Critic(self.EMBEDDING_DIM, self.CRITIC_HIDDEN_DIM, self.state_size, self.CRITIC_LR, self.TAU)\n",
        " \n",
        "    self.embedding_network = UserMovieEmbedding(max(self.users)+1, max(self.items)+1, self.EMBEDDING_DIM)\n",
        "    #self.embedding_network.load_state_dict(torch.load('saved_concat_model'))\n",
        "\n",
        "    #state representation of user and item embeddings\n",
        "    self.state_repr = InnerProductLayer(num_inputs=len(self.items))\n",
        "\n",
        "    #initialize PER buffer\n",
        "    self.buffer = PriorityExperienceReplay(self.REPLAY_MEMORY_SIZE, self.EMBEDDING_DIM, self.state_size)\n",
        "    \n",
        "    self.epsilon = 1\n",
        "    self.std = 1.5\n",
        "\n",
        "  def recommend_item(self, action, recommended_items, top_k=False, item_ids=None):\n",
        "    if item_ids == None:\n",
        "      item_ids = np.array(list(set(i for i in self.items) - recommended_items))\n",
        "\n",
        "    #item embdedings\n",
        "    item_ebs = self.embedding_network.m(torch.LongTensor(list(item_ids)))\n",
        "    action = torch.transpose(action, 0, 1)\n",
        "\n",
        "    if top_k:\n",
        "      pass\n",
        "    else:\n",
        "      product = torch.mm(item_ebs, action)\n",
        "      item_idx = torch.argmax(product, axis=0)\n",
        "      return item_ids[item_idx]\n",
        "\n",
        "  def calculate_td_targets(self, rewards, q_values, dones):\n",
        "    y_t = torch.clone(q_values)\n",
        "    for i in range(q_values.shape[0]):\n",
        "      y_t[i] = rewards[i] + (1-dones[i])*self.DISCOUNT_FACTOR*q_values[i]\n",
        "\n",
        "    return y_t\n",
        "\n",
        "  def train(self, max_episode_num, top_k=False):\n",
        "\n",
        "    self.actor.update_target_network()\n",
        "    self.critic.update_target_network()\n",
        "\n",
        "    for episode in range(max_episode_num):\n",
        "      #init variables\n",
        "      value_loss = 0\n",
        "      episode_reward = 0\n",
        "      correct_count = 0\n",
        "      steps = 0\n",
        "      mean_action = 0\n",
        "\n",
        "      user_id, items_ids, done = self.env.reset()\n",
        "      while not done:\n",
        "        #get user embedding\n",
        "        user_embedding = self.embedding_network.u(torch.LongTensor([user_id]))\n",
        "\n",
        "        #get item embeddings\n",
        "        item_embeddings = self.embedding_network.m(torch.LongTensor(list(items_ids)))\n",
        "\n",
        "        #get state representation\n",
        "        state = self.state_repr(item_embeddings.unsqueeze(1), user_embedding)\n",
        "\n",
        "        action = self.actor.local_network(state)\n",
        "\n",
        "        #epsilon-greedy exploration\n",
        "        if self.epsilon > np.random.uniform():\n",
        "          #epsilon decay?\n",
        "          action += torch.randn(size=action.shape)\n",
        "\n",
        "        #Recommended item\n",
        "        recommended_item = self.recommend_item(action, self.env.recommended_items, top_k=top_k)\n",
        "        print('rec item:', recommended_item)\n",
        "\n",
        "        #observe new state and get reward\n",
        "        next_item_ids, reward, done, _ = self.env.step(recommended_item, top_k=top_k)\n",
        "        if top_k:\n",
        "          reward = np.sum(reward)\n",
        "\n",
        "        #get next item embedding\n",
        "        next_items_embeddings = self.embedding_network.m(torch.LongTensor(next_item_ids))\n",
        "\n",
        "        #get next state representation\n",
        "        next_state = self.state_repr(next_items_embeddings.unsqueeze(1), user_embedding) #add one additional dimension because of the input shape\n",
        "\n",
        "        #add in buffer current transition\n",
        "        self.buffer.append(state, action, reward, next_state, done)\n",
        "\n",
        "        if self.buffer.crt_idx > 1 or self.buffer.is_full:\n",
        "\n",
        "          #sample minibatch\n",
        "          batch_states, batch_actions, batch_rewards, batch_next_states, batch_dones, \\\n",
        "            weight_batch, index_batch = self.buffer.sample(self.BATCH_SIZE)\n",
        "\n",
        "          batch_target_next_actions = self.actor.target_network(batch_states)\n",
        "          next_q_values = self.critic.target_network(batch_target_next_actions, batch_states)\n",
        "\n",
        "          #calculate td targets\n",
        "          td_targets = self.calculate_td_targets(batch_rewards, next_q_values, batch_dones)\n",
        "\n",
        "          #update priority\n",
        "          for (p, i) in zip(td_targets, index_batch):\n",
        "            self.buffer.update_priority(abs(p[0]) + self.EPSILON_FOR_PRIORITY, i)\n",
        "\n",
        "          # batch_actions?\n",
        "          value_loss += self.critic.train(batch_actions, batch_states, td_targets, weight_batch)\n",
        "\n",
        "          self.actor.optimizer.zero_grad()\n",
        "          actions = self.actor.local_network(batch_states)\n",
        "          policy_loss = -self.critic(actions, batch_states)\n",
        "          policy_loss = policy_loss.mean()\n",
        "\n",
        "          policy_loss.backward()\n",
        "          self.actor.optimizer.step()\n",
        "\n",
        "          #Soft update\n",
        "          self.actor.update_target_network()\n",
        "          self.critic.update_target_network()\n",
        "\n",
        "        item_ids = next_item_ids\n",
        "        episode_reward += reward\n",
        "\n",
        "        mean_action += torch.sum(action[0])/(len(action[0]))\n",
        "        steps += 1\n",
        "\n",
        "        if reward > 0:\n",
        "          correct_count += 1\n"
      ],
      "execution_count": 126,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "AWU_5WaiRMwc",
        "outputId": "0e166ae3-d944-4735-a45b-bbc858b0f7d6"
      },
      "source": [
        "\n",
        "data = Dataset.load_builtin('ml-1m')\n",
        "#trainset = data.build_full_trainset()\n",
        "#users_num = trainset.n_users\n",
        "#items_num = trainset.n_items\n",
        "\n",
        "df = pd.DataFrame(data.raw_ratings, columns = ['UserId', 'MovieId', 'Rating',  'Timestamp'], dtype='int32')\n",
        "df = df.astype('int32')\n",
        "users = df['UserId'].unique()\n",
        "items = df['MovieId'].unique()\n",
        "\n",
        "#Arranged in order of the movies watched by users\n",
        "users_dict = np.load('user_dict.npy', allow_pickle=True)\n",
        "\n",
        "\n",
        "#Movie history length for each user\n",
        "users_history_lens = np.load('users_histroy_len.npy')\n",
        "\n",
        "# Training setting\n",
        "train_users_num = int(len(users) * 0.8)\n",
        "train_items_num = len(items) \n",
        "train_users_dict = {k:users_dict.item().get(k) for k in range(1, train_users_num+1)}\n",
        "train_users_history_lens = users_history_lens[:train_users_num]\n",
        "\n",
        "STATE_SIZE = 10\n",
        "MAX_EPISODE_NUM = 8000\n",
        "\n",
        "env = Env(train_users_dict, train_users_history_lens, STATE_SIZE)\n",
        "\n",
        "print('before algo')\n",
        "recommender = Recommender (env, users, items, STATE_SIZE)\n",
        "\n",
        "print('before train')\n",
        "recommender.train(MAX_EPISODE_NUM)"
      ],
      "execution_count": 127,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "before algo\n",
            "before train\n",
            "num_inputs 10\n",
            "result:  torch.Size([1, 5500])\n",
            "Actor state shape: torch.Size([1, 5500])\n",
            "rec item: 1593\n",
            "action: 1593\n",
            "num_inputs 10\n",
            "result:  torch.Size([1, 5500])\n",
            "num_inputs 10\n",
            "result:  torch.Size([1, 5500])\n",
            "Actor state shape: torch.Size([1, 5500])\n",
            "rec item: 1379\n",
            "action: 1379\n",
            "num_inputs 10\n",
            "result:  torch.Size([1, 5500])\n",
            "Actor state shape: torch.Size([32, 5500])\n",
            "state: torch.Size([32, 5500])\n",
            "state: torch.Size([32, 5500])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/autograd/__init__.py:149: UserWarning: Error detected in TanhBackward. No forward pass information available. Enable detect anomaly during forward pass for more information. (Triggered internally at  /pytorch/torch/csrc/autograd/python_anomaly_mode.cpp:85.)\n",
            "  allow_unreachable=True, accumulate_grad=True)  # allow_unreachable flag\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-127-40e1454353a4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'before train'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 34\u001b[0;31m \u001b[0mrecommender\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mMAX_EPISODE_NUM\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-126-2f369b8322f6>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, max_episode_num, top_k)\u001b[0m\n\u001b[1;32m    125\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    126\u001b[0m           \u001b[0;31m# batch_actions?\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 127\u001b[0;31m           \u001b[0mvalue_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcritic\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_actions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_states\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtd_targets\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    128\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    129\u001b[0m           \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mactor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-125-562943f302a2>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, actions, states, td_targets, weight_batch)\u001b[0m\n\u001b[1;32m     55\u001b[0m       \u001b[0;31m#loss = torch.mean(weight_batch*loss)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     56\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 57\u001b[0;31m       \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     58\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    253\u001b[0m                 \u001b[0mcreate_graph\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    254\u001b[0m                 inputs=inputs)\n\u001b[0;32m--> 255\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    256\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    257\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    147\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m    148\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 149\u001b[0;31m         allow_unreachable=True, accumulate_grad=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m    150\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    151\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: one of the variables needed for gradient computation has been modified by an inplace operation: [torch.FloatTensor [1, 100]], which is output 0 of TanhBackward, is at version 1; expected version 0 instead. Hint: the backtrace further above shows the operation that failed to compute its gradient. The variable in question was changed in there or anywhere later. Good luck!"
          ]
        }
      ]
    }
  ]
}