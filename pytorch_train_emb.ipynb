{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "sensitive-cleaning",
   "metadata": {},
   "outputs": [],
   "source": [
    "from surprise import Dataset\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "soviet-mills",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current Time = 02:11:52\n"
     ]
    }
   ],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "now = datetime.now()\n",
    "\n",
    "current_time = now.strftime(\"%H:%M:%S\")\n",
    "print(\"Current Time =\", current_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "loved-powell",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = Dataset.load_builtin('ml-1m')\n",
    "df = pd.DataFrame(data.raw_ratings, columns = ['UserId', 'MovieId', 'Rating',  'Timestamp'])\n",
    "user_movie_rating_df = df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "frequent-harvey",
   "metadata": {},
   "outputs": [],
   "source": [
    "users = df['UserId'].unique()\n",
    "items = df['MovieId'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "suburban-australia",
   "metadata": {},
   "outputs": [],
   "source": [
    "np_users = np.array(list(map(lambda user: int(user), users)), dtype = np.int32)\n",
    "np_items = np.array(list(map(lambda item: int(item), items)), dtype = np.int32)\n",
    "np_labels = np.array(list(map(lambda r: 1 if int(r) > 3 else 0, df['Rating'].unique())), dtype = np.int32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "graduate-monte",
   "metadata": {},
   "outputs": [],
   "source": [
    "EMBEDDING_SIZE = 100\n",
    "\n",
    "MAX_EPOCH = 128\n",
    "INIT_USER_BATCH_SIZE = random.randint(32, 1024)\n",
    "FINAL_USER_BATCH_SIZE = 2048\n",
    "INIT_USER_BATCH_SIZE = random.randint(64, 2048)\n",
    "FINAL_USER_BATCH_SIZE = 4096"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "least-privilege",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_rating = max(np_labels)\n",
    "min_rating = min(np_labels)\n",
    "\n",
    "class UserMovieEmbedding(nn.Module):\n",
    "    def __init__(self, n_users, n_movies, n_factors = 100, nh = 20, p1 = 0.05, p2= 0.5):\n",
    "        super().__init__()\n",
    "        self.u = nn.Embedding(n_users, n_factors)\n",
    "        self.u.weight.data.uniform_(-0.01,0.01)\n",
    "        self.m = nn.Embedding(n_movies, n_factors)\n",
    "        self.m.weight.data.uniform_(-0.01,0.01)\n",
    "        self.lin1 = nn.Linear(n_factors*2, nh)  # bias is True by default\n",
    "        self.lin2 = nn.Linear(nh, 1)\n",
    "        self.drop1 = nn.Dropout(p = p1)\n",
    "        self.drop2 = nn.Dropout(p = p2)\n",
    "    \n",
    "    def forward(self, users, movies): # forward pass i.e.  dot product of vector from movie embedding matrixx\n",
    "                                    # and vector from user embeddings matrix\n",
    "        \n",
    "        # torch.cat : concatenates both embedding matrix to make more columns, same rows i.e. n_factors*2, n : rows\n",
    "        # u(users) is doing lookup for indexed mentioned in users\n",
    "        # users has indexes to lookup in embedding matrix. \n",
    "        \n",
    "        u2,m2 = self.u(users) , self.m(movies)\n",
    "       \n",
    "        x = self.drop1(torch.cat([u2,m2], 1)) # drop initialized weights\n",
    "        x = self.drop2(F.relu(self.lin1(x))) # drop 1st linear + nonlinear wt\n",
    "        r = torch.sigmoid(self.lin2(x)) * (max_rating - min_rating) + min_rating               \n",
    "        return r\n",
    "    \n",
    "    \n",
    "    def get_user_embedding_layer(self, user):\n",
    "        return self.u\n",
    "    \n",
    "    def get_movie_embedding_layer(self, user):\n",
    "        return self.m\n",
    "# class UserMovieEmbedding(nn.Module):\n",
    "#     def __init__(self, users_count, items_count, embedding_dim):\n",
    "#         super().__init__()\n",
    "#         self.user_embeddings = nn.Embedding(users_count, embedding_dim)\n",
    "#         self.item_embeddings = nn.Embedding(items_count, embedding_dim)\n",
    "#         self.output_layer = nn.Linear(FINAL_USER_BATCH_SIZE, 1)\n",
    "        \n",
    "#     def forward(self, users, items):\n",
    "#         user_embedding = self.user_embeddings(users)\n",
    "#         item_embeddings = self.item_embeddings(items)\n",
    "#         mat_mul = torch.matmul(user_embedding, item_embeddings.T)\n",
    "# #         pad = FINAL_USER_BATCH_SIZE - mat_mul.shape[1]\n",
    "# #         if pad != 0:\n",
    "# #             print('MATEMATICA')\n",
    "# #             mat_mul =  torch.cat((mat_mul, torch.zeros(mat_mul.shape[0], pad)), 1)\n",
    "\n",
    "# #         output_padded = torch.cat((mat_mul, torch.zeros()), 1)\n",
    "#         output = torch.sigmoid(self.output_layer(mat_mul))\n",
    "#         return output\n",
    "    \n",
    "#     def get_user_embedding_layer(user):\n",
    "#         return self.user_embeddings\n",
    "    \n",
    "#     def get_movie_embedding_layer(user):\n",
    "#         return self.user_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "allied-bernard",
   "metadata": {},
   "outputs": [],
   "source": [
    "# n_factors = 50\n",
    "# max_rating = max(np_labels)\n",
    "# min_rating = min(np_labels)\n",
    "\n",
    "# def get_emb(ni,nf):\n",
    "#     e = nn.Embedding(ni, nf)\n",
    "#     e.weight.data.uniform_(-0.01,0.01)\n",
    "#     #e.weight.data.normal_(0,0.003)\n",
    "\n",
    "#     return e\n",
    "\n",
    "# class UserMovieEmbedding(nn.Module):\n",
    "#     def __init__(self, n_users, n_movies, nh = 10, p1 = 0.05, p2= 0.5):\n",
    "#         super().__init__()\n",
    "#         (self.u, self.m, self.ub, self.mb) = [get_emb(*o) for o in [\n",
    "#             (n_users, n_factors), (n_movies, n_factors),\n",
    "#             (n_users,1), (n_movies,1)\n",
    "#         ]]\n",
    "        \n",
    "#         self.lin1 = nn.Linear(n_factors*2, nh)  # bias is True by default\n",
    "#         self.lin2 = nn.Linear(nh, 1)\n",
    "#         self.drop1 = nn.Dropout(p = p1)\n",
    "#         self.drop2 = nn.Dropout(p = p2)\n",
    "    \n",
    "#     def forward(self, users, movies): # forward pass i.e.  dot product of vector from movie embedding matrixx\n",
    "#                                     # and vector from user embeddings matrix\n",
    "        \n",
    "#         # torch.cat : concatenates both embedding matrix to make more columns, same rows i.e. n_factors*2, n : rows\n",
    "#         # u(users) is doing lookup for indexed mentioned in users\n",
    "#         # users has indexes to lookup in embedding matrix. \n",
    "        \n",
    "# #         users,movies = cats[:,0],cats[:,1]\n",
    "#         u2,m2 = self.u(users) , self.m(movies)\n",
    "       \n",
    "#         x = self.drop1(torch.cat((u2,m2), 1)) # drop initialized weights\n",
    "#         x = self.drop2(F.relu(self.lin1(x))) # drop 1st linear + nonlinear wt\n",
    "#         r = torch.sigmoid(self.lin2(x)) * (max_rating - min_rating) + min_rating               \n",
    "#         return r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "instructional-motion",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = UserMovieEmbedding(max(np_users)+1, max(np_items)+1, EMBEDDING_SIZE)\n",
    "# model.load_state_dict(torch.load('saved_model_torch'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "southwest-assist",
   "metadata": {},
   "outputs": [],
   "source": [
    "modified_user_movie_rating_df = user_movie_rating_df.apply(np.int32)\n",
    "index_names = modified_user_movie_rating_df[modified_user_movie_rating_df['Rating']<4].index\n",
    "modified_user_movie_rating_df = modified_user_movie_rating_df.drop(index_names)\n",
    "modified_user_movie_rating_df = modified_user_movie_rating_df.drop('Rating', axis=1)\n",
    "u_m_pairs = modified_user_movie_rating_df.to_numpy()\n",
    "\n",
    "positive_user_movie_dict = {u : [] for u in range(1, max(modified_user_movie_rating_df['UserId'])+1)}\n",
    "for data in modified_user_movie_rating_df.iterrows():\n",
    "    positive_user_movie_dict[data[1][0]].append(data[1][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "premier-champion",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_user_movie_batch(positive_pairs, batch_size, negative_ratio=0.5):\n",
    "    batch = np.zeros((batch_size, 3))\n",
    "    positive_batch_size = batch_size - int(batch_size*negative_ratio)\n",
    "    max_user_id = max(modified_user_movie_rating_df['UserId'])+1\n",
    "    max_movie_id = max(modified_user_movie_rating_df['MovieId'])+1\n",
    "    \n",
    "    while True:\n",
    "        idx = np.random.choice(len(positive_pairs), positive_batch_size)\n",
    "        data = positive_pairs[idx]\n",
    "        for i, d in enumerate(data):\n",
    "            batch[i] = (d[0], d[1], 1)\n",
    "        \n",
    "        while i+1 < batch_size:\n",
    "            u = np.random.randint(1, max_user_id)\n",
    "            m = np.random.randint(1, max_movie_id)\n",
    "            if m not in positive_user_movie_dict[u]:\n",
    "                i += 1\n",
    "                batch[i] = (u, m, 0)\n",
    "        \n",
    "        np.random.shuffle(batch)\n",
    "        yield torch.LongTensor(batch[:,0]), torch.LongTensor(batch[:,1]),torch.LongTensor(batch[:,2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "mathematical-faith",
   "metadata": {},
   "outputs": [],
   "source": [
    "bce = torch.nn.BCELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters())\n",
    "\n",
    "def train_step(users, items, labels):\n",
    "    optimizer.zero_grad()\n",
    "    predictions = torch.flatten(model(users, items))\n",
    "    labels = labels.float()\n",
    "#     predictions = torch.flatten(torch.where(predictions > 0.5, torch.tensor(1.0, requires_grad=True), torch.tensor(0.0, requires_grad=True)))\n",
    "#     print(labels, predictions[0])\n",
    "#     labels.requires_grad=True\n",
    "    loss = bce(predictions, labels)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    return loss.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "automotive-participant",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 epoch, Loss: 0.4140145425212207\n",
      "1 epoch, Loss: 0.3672398751592401\n",
      "2 epoch, Loss: 0.3623041728028545\n",
      "3 epoch, Loss: 0.3609416764534888\n",
      "4 epoch, Loss: 0.3602478338069603\n",
      "5 epoch, Loss: 0.3580698673842383\n",
      "6 epoch, Loss: 0.35705869632666226\n",
      "7 epoch, Loss: 0.3577936437042033\n",
      "8 epoch, Loss: 0.3576170276911532\n",
      "9 epoch, Loss: 0.3569568296680685\n",
      "10 epoch, Loss: 0.3574908880181\n",
      "11 epoch, Loss: 0.35739025070530467\n",
      "12 epoch, Loss: 0.3555609909970252\n",
      "13 epoch, Loss: 0.35650846484254617\n",
      "14 epoch, Loss: 0.3556248411291935\n",
      "15 epoch, Loss: 0.35582328013709336\n",
      "16 epoch, Loss: 0.3547584352190377\n",
      "17 epoch, Loss: 0.35494713729522265\n",
      "18 epoch, Loss: 0.35361618414276935\n",
      "19 epoch, Loss: 0.35364954271277443\n",
      "20 epoch, Loss: 0.3523416558250052\n",
      "21 epoch, Loss: 0.35402903461554014\n",
      "22 epoch, Loss: 0.35138327078741105\n",
      "23 epoch, Loss: 0.3509424330025423\n",
      "24 epoch, Loss: 0.3495912255078066\n",
      "25 epoch, Loss: 0.34875556276958497\n",
      "26 epoch, Loss: 0.34688813630186144\n",
      "27 epoch, Loss: 0.3462115843765071\n",
      "28 epoch, Loss: 0.3459052390983847\n",
      "29 epoch, Loss: 0.3445278160640451\n",
      "30 epoch, Loss: 0.34391319959378636\n",
      "31 epoch, Loss: 0.34302323287139175\n",
      "32 epoch, Loss: 0.3425106108188629\n",
      "33 epoch, Loss: 0.34069839999323986\n",
      "34 epoch, Loss: 0.34049904590747393\n",
      "35 epoch, Loss: 0.3413268350919739\n",
      "36 epoch, Loss: 0.33901768580811925\n",
      "37 epoch, Loss: 0.3387435839557257\n",
      "38 epoch, Loss: 0.3381668612605236\n",
      "39 epoch, Loss: 0.3376875280356798\n",
      "40 epoch, Loss: 0.3364551311633626\n",
      "41 epoch, Loss: 0.33652469381445743\n",
      "42 epoch, Loss: 0.3370751377011909\n",
      "43 epoch, Loss: 0.3362539104018055\n",
      "44 epoch, Loss: 0.33532087236154273\n",
      "45 epoch, Loss: 0.3337378448150197\n",
      "46 epoch, Loss: 0.3333804265397494\n",
      "47 epoch, Loss: 0.33202710417939013\n",
      "48 epoch, Loss: 0.33270383675078874\n",
      "49 epoch, Loss: 0.3308052878643646\n",
      "50 epoch, Loss: 0.3300102608369999\n",
      "51 epoch, Loss: 0.32785660932298566\n",
      "52 epoch, Loss: 0.3276641873306915\n",
      "53 epoch, Loss: 0.32751993883828645\n",
      "54 epoch, Loss: 0.32618224669675355\n",
      "55 epoch, Loss: 0.326045426677485\n",
      "56 epoch, Loss: 0.3258152436770377\n",
      "57 epoch, Loss: 0.3247045230914335\n",
      "58 epoch, Loss: 0.32436198386989656\n",
      "59 epoch, Loss: 0.3232780742596407\n",
      "60 epoch, Loss: 0.32346262343105725\n",
      "61 epoch, Loss: 0.32282403111457825\n",
      "62 epoch, Loss: 0.32279167177735785\n",
      "63 epoch, Loss: 0.3221085345158812\n",
      "64 epoch, Loss: 0.32062311409438243\n",
      "65 epoch, Loss: 0.3214125287581663\n",
      "66 epoch, Loss: 0.3205332168545879\n",
      "67 epoch, Loss: 0.3210030085727817\n",
      "68 epoch, Loss: 0.32056350664037175\n",
      "69 epoch, Loss: 0.3194442844537438\n",
      "70 epoch, Loss: 0.3191588750628174\n",
      "71 epoch, Loss: 0.3187660617662258\n",
      "72 epoch, Loss: 0.3180973123087258\n",
      "73 epoch, Loss: 0.3184989557647314\n",
      "74 epoch, Loss: 0.31804364101320015\n",
      "75 epoch, Loss: 0.3186616336957353\n",
      "76 epoch, Loss: 0.3184077087484422\n",
      "77 epoch, Loss: 0.31810618191957474\n",
      "78 epoch, Loss: 0.3176553409363403\n",
      "79 epoch, Loss: 0.3170477545163671\n",
      "80 epoch, Loss: 0.31716028258937307\n",
      "81 epoch, Loss: 0.31694228348673364\n",
      "82 epoch, Loss: 0.3170711989285516\n",
      "83 epoch, Loss: 0.3163153884596512\n",
      "84 epoch, Loss: 0.31593435746235926\n",
      "85 epoch, Loss: 0.3152234622689544\n",
      "86 epoch, Loss: 0.3162772977938417\n",
      "87 epoch, Loss: 0.31610338389873505\n",
      "88 epoch, Loss: 0.31499273842964015\n",
      "89 epoch, Loss: 0.31564317216150095\n",
      "90 epoch, Loss: 0.31522240856143297\n",
      "91 epoch, Loss: 0.31467278924633246\n",
      "92 epoch, Loss: 0.3149976244226831\n",
      "93 epoch, Loss: 0.31418426102790675\n",
      "94 epoch, Loss: 0.31481772697851307\n",
      "95 epoch, Loss: 0.3152799252115312\n",
      "96 epoch, Loss: 0.31430775037065883\n",
      "97 epoch, Loss: 0.31380584799363964\n",
      "98 epoch, Loss: 0.3141014242514235\n",
      "99 epoch, Loss: 0.3144408906580972\n",
      "100 epoch, Loss: 0.31365140625199334\n",
      "101 epoch, Loss: 0.31444720997185005\n",
      "102 epoch, Loss: 0.3138377908311906\n",
      "103 epoch, Loss: 0.3137667712862374\n",
      "104 epoch, Loss: 0.3125277554891149\n",
      "105 epoch, Loss: 0.31326929064559156\n",
      "106 epoch, Loss: 0.3122561885440936\n",
      "107 epoch, Loss: 0.3125620855659735\n",
      "108 epoch, Loss: 0.31289737366262027\n",
      "109 epoch, Loss: 0.3124303947218129\n",
      "110 epoch, Loss: 0.31394776640856853\n",
      "111 epoch, Loss: 0.3122783712432033\n",
      "112 epoch, Loss: 0.31263538633213667\n",
      "113 epoch, Loss: 0.31308815945855906\n",
      "114 epoch, Loss: 0.31133033860413756\n",
      "115 epoch, Loss: 0.31183277021666045\n",
      "116 epoch, Loss: 0.312271352796281\n",
      "117 epoch, Loss: 0.31300495259585925\n",
      "118 epoch, Loss: 0.3116771004727629\n",
      "119 epoch, Loss: 0.31211503973749816\n",
      "120 epoch, Loss: 0.31264435292267406\n",
      "121 epoch, Loss: 0.3125911077759305\n",
      "122 epoch, Loss: 0.31123589138027097\n",
      "123 epoch, Loss: 0.31171432296272183\n",
      "124 epoch, Loss: 0.31170311264815875\n",
      "125 epoch, Loss: 0.31029776911266516\n",
      "126 epoch, Loss: 0.3107361182814739\n",
      "127 epoch, Loss: 0.3101485640299125\n"
     ]
    }
   ],
   "source": [
    "losses = []\n",
    "us, mo, _ = next(generate_user_movie_batch(u_m_pairs, 100))\n",
    "\n",
    "for epoch in range(MAX_EPOCH):\n",
    "    running_loss = 0.0\n",
    "    batch_size = INIT_USER_BATCH_SIZE * (epoch+1)\n",
    "    if batch_size > FINAL_USER_BATCH_SIZE:\n",
    "        batch_size = FINAL_USER_BATCH_SIZE\n",
    "    test_generator = generate_user_movie_batch(u_m_pairs, batch_size)\n",
    "    steps_count = len(user_movie_rating_df)//batch_size\n",
    "    for step in range(steps_count):\n",
    "        # embedding layer update\n",
    "        u_batch, m_batch, u_m_label_batch = next(test_generator)\n",
    "        loss = train_step(u_batch, m_batch, u_m_label_batch)\n",
    "        running_loss += loss\n",
    "    print(f'{epoch} epoch, Loss: {running_loss/steps_count}')\n",
    "    running_loss = 0.0\n",
    "    torch.save(model.state_dict(), 'saved_concat_model')\n",
    "\n",
    "#     test_losses.append(test_train_loss.result())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "indoor-bible",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current Time = 03:17:39\n"
     ]
    }
   ],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "now = datetime.now()\n",
    "\n",
    "current_time = now.strftime(\"%H:%M:%S\")\n",
    "print(\"Current Time =\", current_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "better-reasoning",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = UserMovieEmbedding(max(np_users)+1, max(np_items)+1, EMBEDDING_SIZE)\n",
    "model.load_state_dict(torch.load('saved_concat_model'))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
